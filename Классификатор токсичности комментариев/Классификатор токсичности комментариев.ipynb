{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tqdm import notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv ('toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predproc (review):\n",
    "    review = re.sub(\"[^A-Za-z]\", \" \", review)\n",
    "    review = review.lower()\n",
    "    review = word_tokenize(review)\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    review = [wordnet_lemmatizer.lemmatize(word, pos=\"v\") for word in review if word not in set(stopwords.words(\"english\"))]\n",
    "    review = \" \".join(review)\n",
    "    return review      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32c51504ca54a448d7238567aae5833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=159571.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for i in notebook.tqdm(range(0, len(df))):\n",
    "    review = predproc(df.text[i])\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.DataFrame(corpus)\n",
    "df['lem'] = corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "БЫЛО: Explanation\n",
      "Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "СТАЛО: explanation edit make username hardcore metallica fan revert vandalisms closure gas vote new york dolls fac please remove template talk page since retire\n",
      "=======================================================================================================================\n",
      "БЫЛО: Rex Mundi \n",
      "\n",
      "I've created a stub on Rex Mundi at Rex Mundi High School.  Only thing I know about it is that both my Aunt Donna and Bob Griese went there.  Please add anything you might know about it.\n",
      "\n",
      "BTW, my dad was a Panther; I live in Princeton myself.\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "СТАЛО: rex mundi create stub rex mundi rex mundi high school thing know aunt donna bob griese go please add anything might know btw dad panther live princeton\n",
      "=======================================================================================================================\n",
      "БЫЛО: Hi RedRose and apologies for delay. Here is a draft of what I propose to present on the 'new section' pages.\n",
      "The 6 volumes of Ultimate Allocations cover the 'reported' allocations of all British Railways: Steam  Diesel & Electric Locomotives on BR stock from 1950 - 1968 with additional information from 1948 where available. They are formatted in A4 comb bound volumes for ease of use. There are 3 main columns which are sub-divided with individual locomotive details including date when 'new' if appropriate. All 'reported' and published allocations and re-allocations combined with withdrawal dates where known are listed on either 'Weekly' - 'Monthly' - or 'Period Ending' dependant on regional variations at that time. A link to several testimonials and scanned images type of information provided can be found at www.ultimate-allocations.co.uk\n",
      "Please advise if this information would be acceptable for presentation in the 'new section' you previously advised. I look forward to seeing your reply and thank you once again for your assistance in this matter.\n",
      "Mike McManus.  Mike61680\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "СТАЛО: hi redrose apologies delay draft propose present new section page volumes ultimate allocations cover report allocations british railways steam diesel electric locomotives br stock additional information available format comb bind volumes ease use main columns sub divide individual locomotive detail include date new appropriate report publish allocations allocations combine withdrawal date know list either weekly monthly period end dependant regional variations time link several testimonials scan image type information provide find www ultimate allocations co uk please advise information would acceptable presentation new section previously advise look forward see reply thank assistance matter mike mcmanus mike\n",
      "=======================================================================================================================\n",
      "БЫЛО: I think she meets Notability guidelines and thanks for creating the page for her. She was a big icon in the pageant drag community and in the drag/gay community in general. Her death has been covered by several 'mainstream' media outlets.\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "СТАЛО: think meet notability guidelines thank create page big icon pageant drag community drag gay community general death cover several mainstream media outlets\n"
     ]
    }
   ],
   "source": [
    "print(\"БЫЛО:\", df.text[0])\n",
    "print('-----------------------------------------------------------------------------------------------------------------------')\n",
    "print(\"СТАЛО:\", df.lem[0])\n",
    "print(\"=======================================================================================================================\")\n",
    "print(\"БЫЛО:\", df.text[1000])\n",
    "print('-----------------------------------------------------------------------------------------------------------------------')\n",
    "print(\"СТАЛО:\", df.lem[1000])\n",
    "print(\"=======================================================================================================================\")\n",
    "print(\"БЫЛО:\", df.text[10000])\n",
    "print('-----------------------------------------------------------------------------------------------------------------------')\n",
    "print(\"СТАЛО:\", df.lem[10000])\n",
    "print(\"=======================================================================================================================\")\n",
    "print(\"БЫЛО:\", df.text[100000])\n",
    "print('-----------------------------------------------------------------------------------------------------------------------')\n",
    "print(\"СТАЛО:\", df.lem[100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split (data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train_test_split (train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = train['lem'].values.astype('U')\n",
    "corpus_valid = valid['lem'].values.astype('U')\n",
    "corpus_test = test['lem'].values.astype('U')\n",
    "y_train = train['toxic']\n",
    "y_valid = valid['toxic']\n",
    "y_test = test['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_train = count_tf_idf.fit_transform(corpus_train)\n",
    "tf_idf_valid = count_tf_idf.transform(corpus_valid)\n",
    "tf_idf_test = count_tf_idf.transform(corpus_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вывод: предобработка текста проведена. Проведена лемматизация слов с помощью библиотеки WordNetLemmatizer, убраны стоп слова согласно библиотеки stopwords английского языка, так же удалены все символы, регистр приведен к нижнему. Так же для обучения моделей текст был преобразован в вектора с помощью TfidfVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DummyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### для создания бейслайна адекватности моделей сначала создадим модель DummyClassifier со стратегией \"слепого угадывания\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier (strategy='uniform', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyClassifier(random_state=42, strategy='uniform')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy.fit(tf_idf_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = dummy.predict(tf_idf_train)\n",
    "valid_pred = dummy.predict(tf_idf_valid)\n",
    "test_pred = dummy.predict(tf_idf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на обучающей выборке: 0.16954887829637702\n",
      "F1-score на валидационной выборке: 0.17037326050201587\n",
      "F1-score на тестовой выборке: 0.17157117604242486\n"
     ]
    }
   ],
   "source": [
    "print('F1-score на обучающей выборке:', f1_score (y_train, train_pred))\n",
    "print('F1-score на валидационной выборке:', f1_score (y_valid, valid_pred))\n",
    "print('F1-score на тестовой выборке:', f1_score (y_test, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression (penalty='l1', solver='saga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(penalty='l1', solver='saga')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "logreg.fit(tf_idf_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = logreg.predict(tf_idf_train)\n",
    "valid_pred = logreg.predict(tf_idf_valid)\n",
    "test_pred = logreg.predict(tf_idf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на обучающей выборке: 0.7918524635287641\n",
      "F1-score на валидационной выборке: 0.7778258986574275\n",
      "F1-score на тестовой выборке: 0.765666140073723\n"
     ]
    }
   ],
   "source": [
    "print('F1-score на обучающей выборке:', f1_score (y_train, train_pred))\n",
    "print('F1-score на валидационной выборке:', f1_score (y_valid, valid_pred))\n",
    "print('F1-score на тестовой выборке:', f1_score (y_test, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier (random_state=42, booster='dart', max_depth=20, n_estimators=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 43s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(booster='dart', max_depth=20, n_estimators=150, random_state=42)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "xgb.fit(tf_idf_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = xgb.predict(tf_idf_train)\n",
    "valid_pred = xgb.predict(tf_idf_valid)\n",
    "test_pred = xgb.predict(tf_idf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на обучающей выборке: 0.8619841486745012\n",
      "F1-score на валидационной выборке: 0.751228226887003\n",
      "F1-score на тестовой выборке: 0.7321265313585664\n"
     ]
    }
   ],
   "source": [
    "print('F1-score на обучающей выборке:', f1_score (y_train, train_pred))\n",
    "print('F1-score на валидационной выборке:', f1_score (y_valid, valid_pred))\n",
    "print('F1-score на тестовой выборке:', f1_score (y_test, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'max_depth':(1, 5, 10, 15, 20, 25, 30),\n",
    "        'learning_rate':(0.1, 0.2, 0.3, 0.4, 0.5),\n",
    "        'n_estimators':(50, 100, 150, 200, 250),\n",
    "        'boosting_type':['gbdt', 'dart', 'goss']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = RandomizedSearchCV (LGBMClassifier(), \n",
    "                               param_distributions = param, \n",
    "                               n_iter=100, \n",
    "                               cv=3, \n",
    "                               n_jobs=-1, \n",
    "                               random_state=42,\n",
    "                               verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed: 21.1min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 32.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 33min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=LGBMClassifier(), n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'boosting_type': ['gbdt', 'dart',\n",
       "                                                          'goss'],\n",
       "                                        'learning_rate': (0.1, 0.2, 0.3, 0.4,\n",
       "                                                          0.5),\n",
       "                                        'max_depth': (1, 5, 10, 15, 20, 25, 30),\n",
       "                                        'n_estimators': (50, 100, 150, 200,\n",
       "                                                         250)},\n",
       "                   random_state=42, verbose=1)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time\n",
    "lgbm.fit(tf_idf_train, y_train, eval_set=(tf_idf_valid, y_valid), verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 250,\n",
       " 'max_depth': 30,\n",
       " 'learning_rate': 0.5,\n",
       " 'boosting_type': 'dart'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = lgbm.predict(tf_idf_train)\n",
    "valid_pred = lgbm.predict(tf_idf_valid)\n",
    "test_pred = lgbm.predict(tf_idf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на обучающей выборке: 0.8825577714466604\n",
      "F1-score на валидационной выборке: 0.7848689771766696\n",
      "F1-score на тестовой выборке: 0.7662650602409639\n"
     ]
    }
   ],
   "source": [
    "print('F1-score на обучающей выборке:', f1_score (y_train, train_pred))\n",
    "print('F1-score на валидационной выборке:', f1_score (y_valid, valid_pred))\n",
    "print('F1-score на тестовой выборке:', f1_score (y_test, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = CatBoostClassifier (random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.099349\n",
      "0:\tlearn: 0.5937403\ttest: 0.5938197\tbest: 0.5938197 (0)\ttotal: 660ms\tremaining: 10m 59s\n",
      "250:\tlearn: 0.1366729\ttest: 0.1428809\tbest: 0.1428809 (250)\ttotal: 2m 34s\tremaining: 7m 42s\n",
      "500:\tlearn: 0.1160828\ttest: 0.1306922\tbest: 0.1306922 (500)\ttotal: 5m 10s\tremaining: 5m 8s\n",
      "750:\tlearn: 0.1033601\ttest: 0.1247724\tbest: 0.1247724 (750)\ttotal: 7m 47s\tremaining: 2m 35s\n",
      "999:\tlearn: 0.0955501\ttest: 0.1222068\tbest: 0.1221868 (990)\ttotal: 10m 20s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.1221867809\n",
      "bestIteration = 990\n",
      "\n",
      "Shrink model to first 991 iterations.\n",
      "Wall time: 10min 26s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x1dc16ad8788>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cat.fit(tf_idf_train, y_train, eval_set=(tf_idf_valid, y_valid), early_stopping_rounds=50, use_best_model=True, verbose=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = cat.predict(tf_idf_train)\n",
    "valid_pred = cat.predict(tf_idf_valid)\n",
    "test_pred = cat.predict(tf_idf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score на обучающей выборке: 0.8099146057447045\n",
      "F1-score на валидационной выборке: 0.7650297291345518\n",
      "F1-score на тестовой выборке: 0.7459400938289427\n"
     ]
    }
   ],
   "source": [
    "print('F1-score на обучающей выборке:', f1_score (y_train, train_pred))\n",
    "print('F1-score на валидационной выборке:', f1_score (y_valid, valid_pred))\n",
    "print('F1-score на тестовой выборке:', f1_score (y_test, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### В данном проекте для предсказания токсичности коментария мы применили такие процедуры предобработки текста как лемматизация,удаление знаков препинания и спец. символов при помощи регулярных выражений, а так же выбрасывали стоп слова и переводили текст в векторный вид для обучения моделей.\n",
    "#### В качестве бейслайна был выбран DummyClassifier с стратегией слепого угадывания, который показал очень низкую вероятность отгадать класс, все модели показали много лучший результат, что свидетельствует об их адекватности. В качестве основной модели выбрали LGBM как самый быстро обучающийся бустер дающий необходимую точность из условий задания, альтернативно можно использовать логистическую регрессию, которая так же показала отличные результаты в данной задаче."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
